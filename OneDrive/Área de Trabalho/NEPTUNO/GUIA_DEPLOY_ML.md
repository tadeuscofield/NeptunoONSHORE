# üöÄ Guia de Deploy - NEPTUNO com Machine Learning

## üéØ Estrat√©gia Recomendada: Deploy H√≠brido

### Arquitetura:
```
Frontend (Vercel)              Backend ML (Railway/Render)
http://neptuno.vercel.app ‚Üê‚Üí https://neptuno-ml.railway.app
```

---

## üì¶ Op√ß√£o 1: Railway.app (RECOMENDADO)

### Vantagens:
‚úÖ Suporta Python + Flask
‚úÖ Deploy autom√°tico via GitHub
‚úÖ Gr√°tis: $5 cr√©dito/m√™s (suficiente para come√ßar)
‚úÖ F√°cil de usar
‚úÖ HTTPS autom√°tico

### Passo a Passo:

#### 1. Preparar Backend para Deploy

Criar `ml/requirements.txt`:
```txt
flask==3.0.0
flask-cors==4.0.0
scikit-learn==1.3.2
numpy==1.26.2
pandas==2.1.3
gunicorn==21.2.0
```

Criar `ml/Procfile`:
```
web: cd api && gunicorn ml_api:app
```

Criar `ml/runtime.txt`:
```
python-3.11.6
```

#### 2. Deploy no Railway

1. Acesse: https://railway.app
2. Conecte com GitHub
3. "New Project" ‚Üí "Deploy from GitHub repo"
4. Selecione pasta `ml/`
5. Railway detecta Python automaticamente
6. Aguarde deploy (2-3 minutos)
7. Copie URL gerada: `https://neptuno-ml.railway.app`

#### 3. Atualizar Frontend

Edite `PDICalculosML.js`:
```javascript
// Antes:
const ML_API_URL = 'http://localhost:5000/api/ml';

// Depois:
const ML_API_URL = 'https://neptuno-ml.railway.app/api/ml';
```

#### 4. Deploy Frontend no Vercel

```bash
cd "C:\Users\tadec\OneDrive\√Årea de Trabalho\NEPTUNO"
vercel deploy
```

---

## üì¶ Op√ß√£o 2: Render.com

### Vantagens:
‚úÖ Suporta Python + Flask
‚úÖ Gr√°tis at√© 750h/m√™s
‚úÖ HTTPS autom√°tico
‚úÖ CI/CD autom√°tico

### Passo a Passo:

1. Acesse: https://render.com
2. "New Web Service" ‚Üí GitHub repo
3. Configure:
   - **Build Command**: `pip install -r requirements.txt`
   - **Start Command**: `cd api && gunicorn ml_api:app`
   - **Environment**: Python 3.11
4. Deploy autom√°tico

---

## üì¶ Op√ß√£o 3: Tudo no Render (Frontend + Backend)

Se quiser tudo em um lugar:

### Estrutura:
```
render.yaml:
services:
  - type: web
    name: neptuno-ml-api
    env: python
    buildCommand: cd ml && pip install -r requirements.txt
    startCommand: cd ml/api && gunicorn ml_api:app

  - type: static
    name: neptuno-frontend
    buildCommand: echo "Static files"
    staticPublishPath: .
```

---

## üîß Modifica√ß√µes Necess√°rias

### 1. Criar `ml/requirements.txt`
```txt
flask==3.0.0
flask-cors==4.0.0
scikit-learn==1.3.2
numpy==1.26.2
pandas==2.1.3
gunicorn==21.2.0
```

### 2. Atualizar `ml/api/ml_api.py`

Adicionar no final:
```python
if __name__ == '__main__':
    # Desenvolvimento
    app.run(host='0.0.0.0', port=5000, debug=True)
else:
    # Produ√ß√£o (Gunicorn)
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))
```

### 3. Configurar CORS para produ√ß√£o

Em `ml/api/ml_api.py`:
```python
# Antes:
CORS(app)

# Depois:
CORS(app, origins=[
    'http://localhost:8000',
    'https://neptuno.vercel.app',  # Seu dom√≠nio Vercel
    'https://*.vercel.app'
])
```

---

## üåê Vari√°veis de Ambiente

### No Railway/Render:
```
FLASK_ENV=production
PORT=5000
```

### No Vercel (frontend):
```
VITE_ML_API_URL=https://neptuno-ml.railway.app/api/ml
```

---

## ‚úÖ Checklist de Deploy

### Backend (Railway/Render):
- [ ] Criar `requirements.txt`
- [ ] Criar `Procfile` (Railway) ou configurar Start Command (Render)
- [ ] Atualizar CORS com dom√≠nio de produ√ß√£o
- [ ] Fazer deploy
- [ ] Testar endpoint: `https://[seu-app].railway.app/api/ml/health`

### Frontend (Vercel):
- [ ] Atualizar `ML_API_URL` em `PDICalculosML.js`
- [ ] Fazer deploy: `vercel deploy --prod`
- [ ] Testar no navegador

---

## üí∞ Custos

| Plataforma | Gr√°tis | Pago |
|------------|--------|------|
| **Vercel** | 100GB bandwidth/m√™s | $20/m√™s (Pro) |
| **Railway** | $5 cr√©dito/m√™s | $0.000231/min |
| **Render** | 750h/m√™s | $7/m√™s (Starter) |

**Recomenda√ß√£o inicial**: Usar tier gr√°tis at√© validar com clientes.

---

## üîí Seguran√ßa

### Adicionar autentica√ß√£o (Futuro):
```python
from flask import request

@app.before_request
def check_api_key():
    api_key = request.headers.get('X-API-Key')
    if api_key != os.environ.get('API_KEY'):
        return jsonify({'error': 'Unauthorized'}), 401
```

---

## üìä Monitoramento

### Railway/Render Dashboard:
- Logs em tempo real
- Uso de CPU/RAM
- Requisi√ß√µes/segundo
- Erros

### Adicionar logging:
```python
import logging
logging.basicConfig(level=logging.INFO)

@app.route('/api/ml/predict/custo', methods=['POST'])
def predict_custo():
    logging.info(f"Requisi√ß√£o de custo: {request.json}")
    # ...
```

---

## üöÄ Deploy R√°pido (5 minutos)

### 1. Backend no Railway:
```bash
cd ml
# Criar requirements.txt (copiar conte√∫do acima)
git add .
git commit -m "Deploy ML API"
git push

# No Railway:
# - Connect GitHub repo
# - Deploy autom√°tico
```

### 2. Frontend no Vercel:
```bash
cd ..
# Atualizar ML_API_URL em PDICalculosML.js
vercel deploy --prod
```

### 3. Testar:
```
Frontend: https://neptuno.vercel.app
API: https://neptuno-ml.railway.app/api/ml/health
```

---

## ‚ùì FAQ

**P: E se Railway/Render ficar lento?**
R: Upgrade para plano pago ($7-20/m√™s) ou migrar para AWS/GCP.

**P: Modelos .pkl s√£o grandes demais?**
R: Comprima com `pickle.dumps(..., protocol=4)` ou use joblib.

**P: Como fazer deploy autom√°tico?**
R: Conecte GitHub ao Railway/Render. Cada push = deploy autom√°tico.

**P: Preciso de GPU para ML?**
R: N√£o. Esses modelos s√£o leves (Random Forest, Logistic Regression).

---

## üìö Recursos

- Railway Docs: https://docs.railway.app
- Render Docs: https://render.com/docs
- Vercel Docs: https://vercel.com/docs
- Flask Deployment: https://flask.palletsprojects.com/en/3.0.x/deploying/

---

**Pr√≥ximo passo**: Criar `requirements.txt` e fazer primeiro deploy no Railway!
